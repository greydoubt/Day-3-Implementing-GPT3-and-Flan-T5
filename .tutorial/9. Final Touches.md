# Final Touches

Today in Day 3 we:
* Learned how to load and run Flan-T5 with HF Transformers in Replit
* Integrated with Gradio
* Got to know the OpenAI API & integrate
* Learned about the ethical considerations of using large language models

Somethings to think about before joining us for Day 4:
- What other tasks can these capabilities be used for?
- How would we modify our pipeline to summarize text?

Continue through this short course to learn more!

## Checkout AI Camp!
<img src="https://i.imgur.com/cm5IS8V.png" width="100px" height="100px" id="ai-camp">

 **If you are a teenager interested in joining a community solving problems with technology and exploring careers in tech, check out AI Camp!**

  We offer 1-week and 3-week camps to start, but exceptional students can join our [Team Tomorrow](https://teamtomorrow.com/). **_TT members work after school + weekends on paid internal and external projects_**, opening doors for their future. 

  In fact TT member Shriya Dave, a college freshman, helped create today's content!

If you wanted to see our solved files for the GPT3 and FlanT5 models, take a look below: 

```python3
#from transformers import TFAutoModelForSeq2SeqLM, AutoTokenizer
import gradio as gr
import os
import openai

openai.api_key = os.getenv("OPENAI_API_KEY")

#model = TFAutoModelForSeq2SeqLM.from_pretrained("google/flan-t5-small")
#tokenizer = AutoTokenizer.from_pretrained("google/flan-t5-small")


def generate_text_gpt(input_string, max_length):
  response = openai.Completion.create(model="text-davinci-003",
                                        prompt=input_string,
                                      temperature=0,
                                      max_tokens=max_length,
                                      top_p=1,
                                      frequency_penalty=0,
                                      presence_penalty=0)
  answer = response.choices[0]['text']
  return (answer)


def generate_text_flan(input_string, max_length):
  inputs = tokenizer(input_string, return_tensors="pt")
  outputs = model.generate(**inputs, max_length=max_length)
  final_text = tokenizer.batch_decode(outputs[0], skip_special_tokens=True)

  return (final_text)


def to_gradio():
  demo = gr.Interface(fn=generate_text_gpt,
                      inputs=["text", gr.Slider(0, 250)],
                      outputs="text")
  demo.launch(debug=True, share=True)


if __name__ == "__main__":
  to_gradio()
```