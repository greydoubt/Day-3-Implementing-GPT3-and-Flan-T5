# Running Flan-T5
Hopefully you were able to implement inference and an interface on your own but regardless let's do it together!

## Generate Text
Below is our text generation function:
```python
def generate_text_flan(input_string, max_length):
  inputs = tokenizer(input_string, return_tensors="pt")
  outputs = model.generate(**inputs, max_length=max_length)
  final_text = tokenizer.batch_decode(outputs[0], skip_special_tokens=True)

  return (final_text)
```
You can see the similarities between our function and the documentation with only a few minor changes. Namely, we want to be able to customize the length of our output. 


## Gradio interface
This part should be easy, we know we have two inputs, one text, one a number (slider) as well as one text output. 

```python
def to_gradio():
  demo = gr.Interface(fn=generate_text_flan,
                      inputs=["text", gr.Slider(0, 250)],
                      outputs="text")
  demo.launch(debug=True, share=True)
  ```

Now all we need is a main function and to give it a run!
  
```python
if __name__ == "__main__":
  to_gradio()
```

You may notice performance is still not perfect but you should also notice a way faster response time and still some quality outputs. 

Try using our interface for some typical NLP applications like sentiment or topic analysis!

Examples:
1.  **Label the following passage with the topic discussed: AI-Camp is a great way to learn about artificial intelligence through hands on work and join a community of practice.**
2.  **Label the sentiment of the following passage on a scale of 1(very negative) to 5(great!): I thought that AI-Camp was a kind of good experience. Numerical sentiment label:**

