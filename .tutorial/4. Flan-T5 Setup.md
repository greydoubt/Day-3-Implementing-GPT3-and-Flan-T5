# Setting Up Flan-T5


## Transformers
Unlike in previous days when we used the Huggingface Inference API, today we will use the [transformers library](https://huggingface.co/docs/transformers/index) we discussed previously. We are going to run through this quickly because it will not improve performance as we can only fit the small model into our Replit. It will however give us experience loading and running a model locally. Why do you think that is important?

Running models locally gives us full control over them, we do not have to wait for the models to wake-up ([it can take forever](https://huggingface.co/google/flan-t5-xxl)). It is also often the best way to scale as we can have dedicated powerful machines to support our needs. 

## Implementing T5
Lets go ahead and import the necessary packages and load our model [Flan-T5 small](https://huggingface.co/google/flan-t5-small) following instructions to run the model on CPU. (You can [learn more about GPU's and CPU's here](https://www.weka.io/learn/hpc/gpus-for-machine-learning) among many other resources online.)

```python
from transformers import TFAutoModelForSeq2SeqLM, AutoTokenizer
import gradio as gr

model = TFAutoModelForSeq2SeqLM.from_pretrained("google/flan-t5-small")
tokenizer = AutoTokenizer.from_pretrained("google/flan-t5-small")
```
A couple notes:
1. For Flan-T5 the tokenizer (how we encode our inputs and decode our outputs) is separate  to the model. 

2. Typically we would have to install many dependencies in order into our environment to use these packages but Replit handles that for us, it's AWESOME.


**Next we can make a simple text generation function, gradio interface, and a main function.**

**Try this on your own using documentation for [T5 inference](https://huggingface.co/docs/transformers/model_doc/t5#inference) and [Gradio](https://gradio.app/quickstart/) before going to the next page.**